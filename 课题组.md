# 课题组

## 手语理解
1.预处理：提取视频中的手部骨骼、肢体动作、面部表情等特征。
2.特征建模：用GCN、时序模型捕捉时空特征。
3.语义解码：将特征映射为文本序列。

## 手语生成
这一步与手语理解反向，将“自然语言文本”转化为连贯的手语视频或者是骨骼动画，但是我们面临的核心挑战就是动作的自然性和语义的准确性。就类似于，如果语义是高兴的干什么，我们该如何把这个高兴的表现出来，是面部表情还是动作幅度，这都需要我们来思考。

## 句子级手语分割
长段手语视频通常没有明确的分界界限，需要将视频拆分为对应的句子片段，再逐段翻译。

## 端设备智能
是指在手机、智能手表、边缘服务器等非云端设备上运行ai算法，主要的核心诉求就是低延迟，低功耗，隐私保护，这就是为什么想在非云端的设备上运行的原因。（智能感知领域的主流研究场景）

## 疑惑
- 我看到您的团队在CVPR 2024提出的sighgraph，将手语序列建模为图结构来捕捉跨区域特征，这个思路很巧妙，想请教一下，您是如何平衡“手部关键关节”“肢体区域”等不同粒度的空间信息？另外，这种图结构在处理长视频中动作歧义时，相比传统的时序模型有哪些独特的优势？
- 您团队的工作提到了要实现边缘设备的实时手语理解，我了解到轻量级网络和模型压缩是关键。想请问，在优化过程中，您是优先压缩模型数量，还是有限优化推理时候的占用内存？因为移动端场景下，这两者对实时性的影响可能不太相同吧，您的团队是如何权衡的呢？另外，当前模型在普通手机上的推理延迟大概是多少，距离实际落地的阈值还有多大差距呢？
- 您的研究方向里强调多模态感知，在手语理解中，除了手部骨骼和肢体动作，面部表情或者是唇部动作等是否会作为辅助模态融入模型？如果引入了这些模态，跨模态对齐（比如动作帧与表情帧的同步）会不会成为新的挑战？我们目前是如何解决的他们呢？我们目前为止在这方面有没有什么独到的经验呢？
- 对于句子级的手语分割我感觉还好，但是有一点我比较关心的是，每个手语者可能会因为情绪的不同，或者是对手语理解能力的不同，那么大家的手语速度也就会有些区别，这种个体差异对于分割模型的鲁棒性影响大吗？我们是如何处理这种个体的差异的呢？

